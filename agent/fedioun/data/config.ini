[simulation]
#the total number of learning episode
max_episode=80

#the number of test episode at each end of a learning episode
test_episode_per_episode=0

#the total number of testing episode after the learning phase
test_episode_at_end=0
#if you want to test the performance of the agent
#max_episode=0

#dump to file each n episode
dump_log_each=1

#display to standard output each n episode
display_log_each=100

#save the agent each n episode
save_agent_each=10

[environment]
#during one episode, the simulator can iterate over multiple instance
#for instance with a stochastic environment
instance_per_episode=1

#to limit the number of step for one instance
max_step_per_instance=500
apply_armature=true
control=2
damping=0
approx=1
mu=0.8
mu2=-1
soft_cfm=0.02
soft_erp=-1
slip1=-1
slip2=-1
bounce=-1
reward=3



[agent]
gamma=0.99
decision_each=1


#policy
noise=0.05
gaussian_policy=true

hidden_unit_q=50,7
hidden_unit_a=20,10

#learning (to test)
reset_qnn=true
reset_ann=true
inverting_grad=false
nb_actor_updates=25
nb_critic_updates=1
nb_fitted_updates=10
nb_internal_critic_updates=10
no_forgot_offline=false
weighting_strategy=3

#fixed from ddpg:
mini_batch_size=1
#replay_memory=1000000
replay_memory=3000
alpha_a=0.1
alpha_v=0.3
batch_norm=11
decay_v=-1
tau_soft_update=0.1
last_layer_actor=0

#fixed others ideas
max_stabilizer=false
min_stabilizer=false
minibatcher=0
sampling_strategy=0
fishing_policy=0
force_online_update=false
target_network=false
mixed_sampling=false
hidden_layer_type=1
stop_reset=false
only_one_traj=false
only_one_traj_actor=false

keep_weights_wr=false
force_online_update_each=0
add_last_traj_sampling=false

