\subsection{Reinforcement learning}

RL is a framework that models sequential decision
problems where an agent learns to make better decisions while interacting with
the environment. After an agent performs an action, the state changes and the
agent receives a numerical value, called reward, that encodes information
about the quality of the state just reached. The goal of the agent is to maximize
its long-term expected total reward. Because it is possible that actions associated 
with low reward will allow the agent to reach high-reward states, the agent
needs to estimate somehow (by trial-and-error) the value of making an action in
a given state.

The underlying formalism of RL is that of {\em Markov Decision Processes} (MDPs).
An MDP is formally defined as a tuple $\langle S, A, T, R \rangle$ where $S$ is a set of states, $A$ a set of actions, $T : S \times A \times S \rightarrow [0, 1]$ 
are transition probabilities between states
($T(s,a,s')=p(s'|a,s)$ is the probability of reaching state $s'$ from state $s$ after executing action $a$) and 
$R : S \times A \rightarrow \mathbb{R}$ is a reward signal. A \textit{policy} $\pi : S \times A \rightarrow [0, 1]$ encodes how the agent will behave (the probability
to take an action in a given state).
The {\em optimal} policy $\pi^*$ is the policy maximizing the expected discounted reward, that is :
\begin{equation}
\pi^* = \underset{\pi}{\operatorname{arg\,max}}\ \mathbb{E} \Big[ \sum\limits_{t=0}^{\infty} \gamma^t \times R(s_t, \pi_t(s_t)) \Big]
\end{equation}
where $t$ denotes a time step and $0 < \gamma < 1$ is a discount factor.
In MDPs, since $T$ and $R$ are given, an optimal policy $\pi^*$ can be computed offline using dynamic programming (e.g., value iteration, policy                                                                  
iteration) \cite{Howard1960} for instance.

The agent however does not know the transition probabilities $T$ and the reward function $R$.
There are two main approaches for learning an optimal policy in RL: model-based approach and model-free approach.
While {\em model-based} methods aim at learning $T$ and $R$,
{\em model-free methods} are computationally more viable techniques
that perform online learning storing an estimation of $Q : S \times A \rightarrow \mathbb{R}$, the (sequential) values of actions in each state,
or $V : S \rightarrow \mathbb{R}$ the value of each state.

\begin{equation}
 V^\pi(s) = \mathbb{E}_\pi \Big[ \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} \Big| s_t = s \Big]
\end{equation}

\begin{equation}
 Q^\pi(s, a) = \mathbb{E}_\pi \Big[ \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} \Big| s_t = s, a_t = a \Big]
\end{equation}


\subsection{Critic only}

\mytodo{MZ}{Je pense qu'il faudrait peut-etre une introduction un peu plus large des 'critic' avant de presenter FittedQ. Parler des mÃ©thodes d'apprximation en general, car la famille de LSTD est bien etudie pour les critic.}

Fitted Q Iteration \cite{Riedmiller2005} is a value iteration based on a collected data $(s_t, a_t,r_{t+1}, s_{t+1})_{t=\{1,...,N\}}$ of one episode containing $N$ steps.

\begin{equation}
 Q_{k+1} = \underset{Q \in \mathcal{F}}{\text{argmin}} \sum_{t=1}^{N} \Big[ Q(s_t, a_t) - \big( r_{t+1} + \gamma \underset{a' \in A}{\text{ max }} Q_k(s_{t+1}, a') \big) \Big]^2
\end{equation}

\begin{equation}
 Q(s_t, a_t) =  r_{t+1} + \gamma \underset{a' \in A}{\text{ max }} Q_k(s_{t+1}, a')
\end{equation}


\subsection{Actor only}

\subsection{Actor Critic}

\begin{tabular}{|p{3.4cm}|>{\centering\arraybackslash}p{1.2cm}|>{\centering\arraybackslash}p{3cm}|>{\centering\arraybackslash}p{1.8cm}|}
 \hline
 ~\mbox{Algorithms} ~ \mbox{Criterions} & Cacla, CMAES & Fitted Natural Actor-critic, Power & Fitted Q iteration \\ \hline
 1) Continuous Actions & $\times$ & $\times$ & \\ \hline
 2) Low Knowledge & $\times$ & & $\times$ \\ \hline
 3) Data Efficient &  & $\times$ & $\times$\\ \hline
\end{tabular}


\DRAFT{

\cite{Antos2008} provides an extension for continuous actions space.

In this case, the model

importance sampling

\begin{itemize}
 \item Actor only
 \begin{itemize}
  \item CMAES
  \item Power
 \end{itemize}
 \item Actor critic
 \begin{itemize}
  \item Cacla
  \item Fitted Q iteration
  \item Fitted Natural Actor Critic 
 \end{itemize}
 \item Critic only : problem with continuous action space
\end{itemize}

We'll present 3 state of the art algorithms that fulfill 2 different hypothesis. \\
1) 2) cacla, CMAES ?\\
1) 3) fitted natural actor critic, Power\\
2) 3) fitted q iteration\\
}

