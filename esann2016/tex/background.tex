\subsection{Reinforcement learning}

RL is a framework that models sequential decision
problems where an agent learns to make better decisions while interacting with
the environment. After an agent performs an action, the state changes and the
agent receives a numerical value, called reward, that encodes information
about the quality of the state just reached. The goal of the agent is to maximize
its long-term expected total reward. Because it is possible that actions associated 
with low reward will allow the agent to reach high-reward states, the agent
needs to estimate somehow (by trial-and-error) the value of making an action in
a given state.

The underlying formalism of RL is that of {\em Markov Decision Processes} (MDPs).
An MDP is formally defined as a tuple $\langle S, A, T, R \rangle$ where $S$ is a set of states, $A$ a set of actions, $T : S \times A \times S \rightarrow [0, 1]$ 
are transition probabilities between states
($T(s,a,s')=p(s'|a,s)$ is the probability of reaching state $s'$ from state $s$ after executing action $a$) and 
$R : S \times A \rightarrow \mathbb{R}$ is a reward signal. A \textit{policy} $\pi : S \times A \rightarrow [0, 1]$ encodes how the agent will behave (the probability
to take an action in a given state).
The {\em optimal} policy $\pi^*$ is the policy maximizing the expected discounted reward, that is :
\begin{equation}
\pi^* = \underset{\pi}{\operatorname{arg\,max}}\ \mathbb{E} \Big[ \sum\limits_{t=0}^{\infty} \gamma^t \times R(s_t, \pi_t(s_t)) \Big]
\end{equation}
where $t$ denotes a time step and $0 < \gamma < 1$ is a discount factor.
In MDPs, since $T$ and $R$ are given, an optimal policy $\pi^*$ can be computed offline using dynamic programming (e.g., value iteration, policy                                                                  
iteration) \cite{Howard1960} for instance.

The agent however does not know the transition probabilities $T$ and the reward function $R$.
There are two main approaches for learning an optimal policy in RL: model-based approach and model-free approach.
While {\em model-based} methods aim at learning $T$ and $R$,
{\em model-free methods} are computationally more viable techniques
that perform online learning storing an estimation of $Q : S \times A \rightarrow \mathbb{R}$, the (sequential) values of actions in each state,
or $V : S \rightarrow \mathbb{R}$ the value of each state.

In a small and discrete environment it is possible to store Q or V as a table.
Notice however that in many tasks, the number of states and actions are
too large so that the agent cannot store it in a table. The problem of
continuous states can be tackled with discretization; however the number of
states will often explode. Even with the memory and computational power to
handle very large tables, reinforcement learning with Q values represented in the
tabular form suffers from the lack of generalization, meaning that what is learned
in a state does not impact the knowledge about the value of similar states.
A solution to these problems is to use an approximation function, that provide
a means for both efficient computation and support for generalization.


% \begin{equation}
%  V^\pi(s) = \mathbb{E}_\pi \Big[ \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} \Big| s_t = s \Big]
% \end{equation}
% 
% \begin{equation}
%  Q^\pi(s, a) = \mathbb{E}_\pi \Big[ \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} \Big| s_t = s, a_t = a \Big]
% \end{equation}


\subsection{Critic only}
La caractéristique ppale de ces méthodes est de dériver une politique directement à partir des fonctions
de valeurs. Elles ont été le plus étudié car fonctionnent bien dans des les environemenst discrets.
On peut citer (QLearning, Sarsa, LSTD)

Nous allons décrire une méthode 

Thoses methods
widely studied
As RL methods has been well studied for discretization
The policy is entierely determined by the value fonction.


The main 

\mytodo{MZ}{Je pense qu'il faudrait peut-etre une introduction un peu plus large des 'critic' avant de presenter FittedQ. Parler des méthodes d'apprximation en general, car la famille de LSTD est bien etudie pour les critic.}

Fitted Q Iteration \cite{Riedmiller2005} is a value iteration based on a collected data $(s_t, a_t,r_{t+1}, s_{t+1})_{t=\{1,...,N\}}$ of one episode containing $N$ steps.

\begin{equation}
 Q_{k+1} = \underset{Q \in \mathcal{F}}{\text{argmin}} \sum_{t=1}^{N} \Big[ Q(s_t, a_t) - \big( r_{t+1} + \gamma \underset{a' \in A}{\text{ max }} Q_k(s_{t+1}, a') \big) \Big]^2
\end{equation}

% \begin{equation}
%  Q(s_t, a_t) =  r_{t+1} + \gamma \underset{a' \in A}{\text{ max }} Q_k(s_{t+1}, a')
% \end{equation}

\begin{equation}
 \Delta W = - \eta \big( Q(s_t, a_t) - r_{t+1} + \gamma Q_k(s_{t+1}, \pi(s_{t+1}) \big) \frac{\partial Q(s_t, a_t)}{\partial W}
\end{equation}

Le problème principale de ces méthodes est qu'elles ne sont pas applicable avec des actions continues
car repose sur l'énumération de toutes les actions possibles. Ce qui n'est pas envisageable dans un
espace continue à grande dimensions.

\subsection{Actor only}

de l'autre cote il est possible de ne pas utiliser du tout de fonction de valeurs.
Définir une polique 

mise à jour par exemple à partir de $ \frac{\partial J}{\partial \theta} $

On the other hand 
direct policy search

L'inconvénient ppale est la variabilité du cout.

\subsection{Actor Critic}

Those methods tries to combine the advantage of both previous methods.

Parmis eux : cacla et fitted natural actor critic.

Notre algorithme fait partie de cette famille.

\begin{tabular}{|p{3.4cm}|>{\centering\arraybackslash}p{1.2cm}|>{\centering\arraybackslash}p{3cm}|>{\centering\arraybackslash}p{1.8cm}|}
 \hline
 ~\mbox{Algorithms} ~ \mbox{Criterions} & Cacla, CMAES & Fitted Natural Actor-critic, Power & Fitted Q iteration \\ \hline
 1) Continuous Actions & $\times$ & $\times$ & \\ \hline
 2) Low Knowledge & $\times$ & & $\times$ \\ \hline
 3) Data Efficient &  & $\times$ & $\times$\\ \hline
\end{tabular}


\DRAFT{

\cite{Antos2008} provides an extension for continuous actions space.

In this case, the model

importance sampling

\begin{itemize}
 \item Actor only
 \begin{itemize}
  \item CMAES
  \item Power
 \end{itemize}
 \item Actor critic
 \begin{itemize}
  \item Cacla
  \item Fitted Q iteration
  \item Fitted Natural Actor Critic 
 \end{itemize}
 \item Critic only : problem with continuous action space
\end{itemize}

We'll present 3 state of the art algorithms that fulfill 2 different hypothesis. \\
1) 2) cacla, CMAES ?\\
1) 3) fitted natural actor critic, Power\\
2) 3) fitted q iteration\\
}

