\subsection{Reinforcement learning}

\subsection{Critic only}
Fitted Q Iteration \cite{Riedmiller2005} is a value iteration based on a collected data $(s_t, a_t,r_{t+1}, s_{t+1})_{t=\{1,...,N\}}$ of one episode containing $N$ steps.

\begin{equation}
 Q_{k+1} = \underset{Q \in \mathcal{F}}{\text{argmin}} \sum_{t=1}^{N} \Big[ Q(x_t, a_t) - \big( r_{t+1} + \gamma \underset{a' \in A}{\text{ max }} Q_k(x_{t+1}, a') \big) \Big]^2
\end{equation}

\DRAFT{

\cite{Antos2008} provides an extension for continuous actions space.

In this case, the model

importance sampling

\begin{itemize}
 \item Actor only
 \begin{itemize}
  \item CMAES
  \item Power
 \end{itemize}
 \item Actor critic
 \begin{itemize}
  \item Cacla
  \item Fitted Q iteration
  \item Fitted Natural Actor Critic 
 \end{itemize}
 \item Critic only : problem with continuous action space
\end{itemize}

We'll present 3 state of the art algorithms that fulfill 2 different hypothesis. \\
1) 2) cacla, CMAES ?\\
1) 3) fitted natural actor critic, Power\\
2) 3) fitted q iteration\\
}